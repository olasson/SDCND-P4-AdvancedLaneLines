# **Advanced Lane Lines** 

*by olasson*

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

*This is a revised version of my Advanced Lane Lines project.*

## Project overview

The majority of the project code is located in the folder `code`:

* [`_centroids.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/_centroids.py)
* [`_draw.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/_draw.py)
* [`_error.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/_error.py)
* [`_math.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/_math.py)
* [`_process.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/code/_process.py)
* [`calibration.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/calibration.py)
* [`detect.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/detect.py)
* [`io.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/io.py)
* [`misc.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/misc.py)
* [`plots.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/code/plots.py)

All files starting with `_` are supporting files for `detect.py` which contains the pipeline implementation. 

The main project script is called [`advanced_lane_lines.py`](https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/advanced_lane_lines.py). It contains the implementation of a very simple command line tool.

The images shown in this readme are found in 

* `images/`

The videos of the pipeline result is found in

* `videos/result/`

## Command line arguments

All arguments are technically optional. The program also checks for "illegal" argument combinations to an extent.

See the main script linked above for a detailed list of supported arguments and their function. I will make an effort to specify which commands were used for different parts of the project.

## Camera Calibration

The code for calibrating the camera is located in `calibration.py`. The project expects to find a set of calibration images located in `./images/calibration/`. When the project is first run, it will automatically check if the calibrating file `calibrated.p` exists or not. If it does not exist, it will calibrate the camera and store the result in `./data/calibrated.p` for future use. 

The camera calibrating starts by preparing a set of "base object points" which corresponds to the (x, y, z) coordinates of the chessboard corners in the real world. To simplify, the calibration assumes that the chessboard is fixed at (x, y, z=0), making the base object points the same for each calibration image. 
    
    ...
    # Prepare object points on the form (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
    base_object_points = np.zeros((n_obj_points, 3), np.float32)
    base_object_points[:,:2] = np.mgrid[0:n_icorners_x, 0:n_icorners_y].T.reshape(-1, 2)
    ...

Note that the z-coordinate in each `base_object_points` is zero. Each time a set of corners is detected, a full copy of `base_object_points` will be apended, like so
    
    ...    
    ret, corners = cv2.findChessboardCorners(gray_image, (n_icorners_x, n_icorners_y), None)

    if ret:
        found_corner_points.append(corners)
        found_object_points.append(base_object_points)
    ...

where `corners` is the (x,y) pixel positions of the detected corners in each image. I debugged my camera calibration using the `--debug` flag, which helps to visualize the results of the camera calibration:


<p align="center">
  <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/calibration_debug.png">
</p>

As can be seen, 3/20 images failed, but the calibration was still successful. 

## Pipeline

The pipeline can be divided into three main steps:
1. Pre-processing
2. Lane detection
3. Post-processing

Each step is described in detail below. The pipeline implementation itself is found in `detect.py`. It is wrapped in a Class called `LaneDetector` since it was very useful to keep track of two internal states when running the pipeline on videos. These states and their function are explained in the `Lane detection` section.

### Pre-processing

This step will prepare an image for lane detection.

#### A) Undistort

This step applies the `mtx` and `dist` parameters saved or generated by the camera calibration by using the opencv function `cv2.undistort()`. 

First, here is the original image

<p align="center">
  <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/test/straight_lines1.jpg">
</p>

And here is the same image after undistortion:

<p align="center">
    <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step01_image_undistorted.png">
</p>

Notice that the horizontal lines running accross the image are straigthened out in the undistorted image, which confirms that the calibration has worked. 

#### B) Thresholding 

The thresholding is done by combining two methods, color thresholding and gradient thresholding. All relevant code is found in `_process.py`

The gradient thresholding is applied by  `_threshold_gradient()`. It consists of the following sub steps:

1. Binary threshold of the absolute value of sobel derivative in the x direction and y direction: 
2. Binary threshold of the gradient magnitude of the sobel derivatives. 
3. Binary threshold of the sobel direction. 
4. Binary threshold of the V channel from a HSV image. 

They are then combined to produce the following image:

<p align="center">
    <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step02_gradient_binary.png">
</p>

Next, a separate binary image is produced based on color thresholding. It is applied by `_threshold_color()` and consists of the following steps:
1. Binary threshold of the R-channel from the original RGB image
2. Binary threshold of the L-channel from a HLS version of the RGB image
3. Binary threshold of the S-channel from a HLS version of the RGB image
4. Binary threshold of the V-channel from a HSV verion of the RGB image
5. Binary threshold of the B-channel from a LAB version of the RGB image

They are then combined to produce the following image

<p align="center">
    <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step03_color_binary.png">
</p>

In the pipeline, the gradient binary and the color binary is combined to produce the final thresholded image:

<p align="center">
    <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step05_combined_threshold.png">
</p>

It should be noted that the above image is no longer a binary image, instead it is a grayscale image. The image looks quite "messy", but the next step will ensure that the pipeline only focus on a certain relevant region of the image. 

#### C) Perspective transform

Before the perspective transform is applied, the source (`src`) and destination (`dst`) points are computed by ` _compute_src_and_dst()` like so:

    ...
    src = np.float32([[(src_top_offset - left_intercept) / left_slope, src_top_offset], # Top left
                      [(src_top_offset - right_intercept) / right_slope, src_top_offset], # Top right
                      [(n_rows - src_bottom_offset - left_intercept) / left_slope, n_rows - src_bottom_offset], # Bottom left
                      [(n_rows - src_bottom_offset - right_intercept) / right_slope, n_rows - src_bottom_offset]]) # Bottom right
    ...
    dst = np.float32([[dst_offset, 0], # Top left
                      [n_cols - dst_offset, 0], # Top right
                      [dst_offset, n_rows], # Bottom left
                      [n_cols - dst_offset, n_rows]]) # Bottom right
    ...

The exact definitons of all above constans are found in the function definition in  ` _process.py`. Visualized the regions defined looks like this (src to the left, dst to the right): 

<p align="center">
    <img width="40%" height="40%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step06_src.png">
    <img width="40%" height="40%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step07_dst.png">
</p>

The exact regions were found by trial and error. The perspective transform is then applied to the combined threshold image to produce the final pre-processed image.

### Lane Detection

This step performs the actual lane detection, based on the warped grayscale image from the post-processing. The relevant files are `detect.py, _math.py, _draw.py, _centroids.py, _error.py`.

The entire lane detection revolves around centroids. For the purposes of this project, a single centroid is a triplet of floats:

    centroid = (left_x, right_x, y)

This is simply a slightly more compact representation of two separate points: 

    P_left = (left_x, y)
    P_right = (right_x, y)

Where `P_left` is a point believed to be a point on the left lane, and `P_right` is a point believed to be a point on the right lane. 

The other concept used in the lane detection is the confidence level of a computed centroid. Specifically, each  `left_x` and `right_x` will have a confidence level associated with it, which is simply a scalar on the interval `[0,1]` where 0.0 is a complete lack of confidence, and 1.0 is complete confidence. This will later be used to filter out "bad" lane lines.    

The first step is to try to find a initial set of points. This is  done by the function `_estimate_first_centroid()` defined in `_centroids.py`. Depending on wether or not it has previous data points stored in the centroid buffer it will compute  `left_min_index, left_max_index` and `right_min_index, right_max_index`. In addition it computes
    
    ...
    window_top = int(n_rows * 0.75)
    y = int(n_rows - WINDOW_HEIGHT / 2)
    ...
 
where `window_top` specifies that it will only look for the first centroids in the bottom "slice" of the image and  `y` is simply the y-coordiante in the centroid. Here, a region of the pre-processed image is converted to a 1D array by summing each column to a single value. 
    
    ...
    left_sum = np.sum(gray_warped[window_top:, left_min_index:left_max_index], axis=0)
    ...
    
Next, a convolution is performed between the window signal (defined by the window width) to detect the lane lines. 
    
    ...
    left_signal = np.convolve(window_signal, left_sum)
    
    left_x, left_confidence = _compute_signal_center(left_signal, left_min_index, scale_confidence = False)
    ...

and the same for `right_x`. The `_compute_signal_center()` function computes the signal center which is defined as follows:

    ...
    center = np.argmax(signal) + offset - (WINDOW_WIDTH / 2)
    ...

where the offset shifts the center relative to the overall image. There is also an option to scale the confidence (or not), depending upon the use case. If `scale_confidence = False`, `_compute_signal_center()` will simply assign complete confidence to the detected centroid, otherwise it will scale the confidence relative to the expected max possible signal value. 

The first centroid found is then used as a reference by the `_estimate_centroids()` which will loop over the image in layers and perform convolutions to detect more centroids. Inside the `_estimate_centroids()` function, it looks like this:

    ...
    window_top = int(n_rows - (layer + 1) * WINDOW_HEIGHT)
    window_bottom = int(n_rows - layer * WINDOW_HEIGHT)
    y = int(window_bottom - WINDOW_HEIGHT / 2)
    
    window_sum = np.sum(gray_warped[window_top:window_bottom, :], axis=0)

    conv_signal = np.convolve(window_signal, window_sum)

    left_x, left_confidence = _compute_window_center(conv_signal, n_cols, prev_left_x)
    right_x, right_confidence = _compute_window_center(conv_signal, n_cols, prev_right_x)
    ...

Note that here the confidence is scaled (`scale_confidence = True` by default). The algorithm assumes that `prev_left_x` and `prev_right_x` is never `None` which is why `_estimate_first_centroid()` simply assumes that the first centroid detected is always good. It will then attempt to handle missing values in various ways, which are described in detail in `_estimate_centroids()`, but the end result should always be float values all centroids. 

After `_estimate_centroids()` have looped over the image, the algorithm uses a very simply error detection sub-algorithm to attempt to infer any "bad" lane lines. This sub-algorithm is found in `_error.py` and specifically `_infer_lane_error_code()` which returns `0` is the lane is OK. Any other error code is treated as sufficient to discard the lane. In the event that the lane is bad, the LaneDetector uses the centroids from the previous frames instead
    
    ...
    if (lane_error_code != 0) and (len(self.centroids_buffer) > 0):
        centroids = self.centroids_buffer[-1]
    ...
    
Next, the centroids are used to fit the lanes to polyomials

    ...
    left_fit, right_fit = _fit_lanes(centroids, 1, 1)
    left_fit_scaled, right_fit_scaled = _fit_lanes(centroids, M_PER_PIXELS_Y, M_PER_PIXELS_X)
    ...

where the "scaled" version of the fits are used to compute the curvature and deviation from center in (approximately) real-world units (meters).

### Post-processing

The starting point for the post-processing is the `left_fit, right_fit` from the previous step, and is relatively straigth forward:

    ...
    image_tmp = _draw_lanes(image_undistorted, self.n_rows, left_fit, right_fit)

    image_tmp = _unwarp_image(image_tmp, self.src, self.dst, self.n_rows, self.n_cols)

    lane_image = cv2.addWeighted(image_undistorted, 1.0, image_tmp, 1.0, 0.0)

    _draw_text(lane_image, curvature, deviation)
    ...
    
The resulting `lane_image` will look like this (assuming successful detection):

<p align="center">
    <img width="80%" height="80%" src="https://github.com/olasson/SDCND-T1-P4-AdvancedLaneLines/blob/master/images/result/straight_lines1/step08_lane_image.png">
</p>

## Video results

## Pipeline discussion


